{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3\n",
    "\n",
    "In this lesson we will discuss basic Pandas operation. Pandas is a very powerful library for doing data science. It's has its issues but I've found that in a normal situation with some shouting at Pandas, you're still about 10 times faster if you use it. So let's dive in! We'll do some basic feature extraction and data selection today.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "To demonstrate this statistics crash course, we'll use the [Coronavirus](https://www.kaggle.com/brendaso/2019-coronavirus-dataset-01212020-01262020) dataset. It describes the spread of *2019-nCov* throughout the world on a per day basis.\n",
    "\n",
    "## This lesson\n",
    "\n",
    "When this lesson is over, you can select useful data, clean the data and extract features for doing some analyses. You will learn this analyzing how many confirmed nCov-2019 cases accumulated in provinces and countries.\n",
    "\n",
    "## Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make my plots pretty!\n",
    "sns.set_style(style=\"darkgrid\")\n",
    "sns.set_context(context=\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/2019_nC0v_20200121_20200126 - SUMMARY.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe\n",
    "\n",
    "What's in the dataset? At the very least we can see:\n",
    "\n",
    "|Column | Definition|\n",
    "|-------|-----------|\n",
    "| Province / State of a country | The province in which 2019-nCov was discovered\n",
    "| Country | In which country it was discovered\n",
    "| Date last updated | Every day new numbers are released, so when was the latest update?\n",
    "| Confirmed | How many 2019-nCov cases are confirmed\n",
    "| Suspected | How many 2019-nCov cases are suspected but not yet confirmed\n",
    "| Recovered | How many confirmed 2019-nCov cases have recovered\n",
    "| Deaths | How many confirmed 2019-nCov cases have had a terminal result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country</th>\n",
       "      <th>Date last updated</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Suspected</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>Deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shanghai</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1/21/2020</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yunnan</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1/21/2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beijing</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1/21/2020</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taiwan</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1/21/2020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jilin</td>\n",
       "      <td>Mainland China</td>\n",
       "      <td>1/21/2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>NaN</td>\n",
       "      <td>France</td>\n",
       "      <td>1/26/2020 11:00 AM</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Australia</td>\n",
       "      <td>1/26/2020 11:00 AM</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>1/26/2020 11:00 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>1/26/2020 11:00 AM</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>Ontario</td>\n",
       "      <td>Canada</td>\n",
       "      <td>1/26/2020 11:00 AM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>368 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Province/State         Country   Date last updated  Confirmed  Suspected  \\\n",
       "0         Shanghai  Mainland China           1/21/2020        9.0       10.0   \n",
       "1           Yunnan  Mainland China           1/21/2020        1.0        NaN   \n",
       "2          Beijing  Mainland China           1/21/2020       10.0        NaN   \n",
       "3           Taiwan  Mainland China           1/21/2020        1.0        NaN   \n",
       "4            Jilin  Mainland China           1/21/2020        NaN        1.0   \n",
       "..             ...             ...                 ...        ...        ...   \n",
       "363            NaN          France  1/26/2020 11:00 AM        3.0        NaN   \n",
       "364            NaN       Australia  1/26/2020 11:00 AM        4.0        NaN   \n",
       "365            NaN           Nepal  1/26/2020 11:00 AM        1.0        NaN   \n",
       "366            NaN        Malaysia  1/26/2020 11:00 AM        4.0        NaN   \n",
       "367        Ontario          Canada  1/26/2020 11:00 AM        1.0        NaN   \n",
       "\n",
       "     Recovered  Deaths  \n",
       "0          NaN     NaN  \n",
       "1          NaN     NaN  \n",
       "2          NaN     NaN  \n",
       "3          NaN     NaN  \n",
       "4          NaN     NaN  \n",
       "..         ...     ...  \n",
       "363        NaN     NaN  \n",
       "364        NaN     NaN  \n",
       "365        NaN     NaN  \n",
       "366        NaN     NaN  \n",
       "367        NaN     NaN  \n",
       "\n",
       "[368 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics table\n",
    "\n",
    "What can we derive from the statistics table? At the very least we can see that there are three columns that aren't in the table: `Province/State`, `Country` and `Date last updated`. Why would that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There are 30 rows that have no entry in the `Confirmed` column.\n",
    "2. As with any \"good\" infectious disease, nCov-2019 cases scale exponentially.\n",
    "\n",
    "How can we see statement 2 in the statistics table?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates and times\n",
    "\n",
    "Let's take a look at one of the most infamous problems in programming: the handling of dates and times. As you can see, this dataset is really no exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['Date last updated'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Mount Everest\n",
    "\n",
    "Sort all the datetimes in the same format from first to last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop it like it's ... unnecessary?\n",
    "\n",
    "Sooo. The dates and times are really hard. Let's just skip those, right?\n",
    "\n",
    "You can easily drop columns in a DataFrame with the `drop()` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Drop the `Date last updated` column and put the result in `clean_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing values\n",
    "\n",
    "Disclaimer: this section is a little political. It exists just to explain the concept of data cleaning, nothing more.\n",
    "\n",
    "Taiwan, Hong Kong and Mainland China are officially part of China. So let us replace all of those values with \"China\", *in place*. That means that we will not return a new DataFrame but rather just edit the current one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Find the documentation for the replace command of a Pandas DataFrame and see what you need to do to replace something *in place*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Replace all occurences of \"Taiwan\", \"Hong Kong\" and \"Mainland China\" to \"China\" in the `Country` column. Do this with an *in place* replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a new dataframe with feature extraction\n",
    "\n",
    "Let's create a new DataFrame. Why? We're only interested in the accumulated suspected/confirmed/recovered/death cases of 2019-nCov. It's hard to put this in the existing DataFrame, because everything is unsorted and we would mess up our data by accumulating everything.\n",
    "\n",
    "We want the new DataFrame to have the following structure. Note that in this process, we have silenty also removed the `Province/State` column:\n",
    "\n",
    "| Country  | Suspected | Confirmed | Recovered | Death |\n",
    "|----------|-----------|-----------|-----------|-------|\n",
    "| China    | ...       | ...       | ...       | ...   |\n",
    "| France   | ...       | ...       | ...       | ...   |\n",
    "| Thailand | ...       | ...       | ...       | ...   |\n",
    "| ...      | ...       | ...       | ...       | ...   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Find a way to determine the totals of `Suspected`, `Confirmed`, `Recovered` and `Deaths` for each `Country` in `clean_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Create a new dataframe (`totals_df`) with **just the column names** given in the description above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Combine the results of Exercise 4 and 5 to fill up the new `totals_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Now we have thrown away some data (the dates and times, and provinces), cleaned some features (\"Mainland China\" -> \"China\") and extracted some features (accumulated cases).\n",
    "\n",
    "Note that even though *feature extraction* sounds really awesome, it usually means something like \"to calculate a value based on values we already have\". Which is exactly what we did: we had all the values for a given datetime, and we accumulated all of them to gain insight in some data you didn't immediately have access to. For example, extracting weekends or Mondays out of datetimes is also feature extraction. You will be amazed by how much information is hidden in these kind of easy-to-extract features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "Create a bar graph for each of the column `Suspected`, `Confirmed`, `Recovered` and `Deaths`. Put the `Country` on the x-axis and the other columns on the y-axis. So the result is 4 bar graphs for each of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
